<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, maximum-scale=1.0">
    <title>AI Interview</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@400;500;600;700;800&display=swap');

        body {
            font-family: 'Plus Jakarta Sans', sans-serif;
            background-color: #020617; /* Slate 950 */
            overflow: hidden;
        }

        .modal-overlay {
            background: rgba(15, 23, 42, 0.8);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
        }

        .glass-card {
            background: rgba(30, 41, 59, 0.6);
            border: 1px solid rgba(51, 65, 85, 0.5);
        }

        .video-wrapper {
            position: relative;
            width: 100%;
            height: 100%;
            background-color: #020617;
        }

        .video-wrapper video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .ai-avatar {
            width: 72px;
            height: 72px;
            border: 3px solid transparent;
            transition: all 0.4s cubic-bezier(0.25, 1, 0.5, 1);
            box-shadow: 0 0 15px rgba(79, 70, 229, 0.3);
        }
        .ai-avatar.speaking {
            border-color: #6366F1;
            box-shadow: 0 0 30px rgba(99, 102, 241, 0.6);
            transform: scale(1.05);
        }

        .transcript-panel {
             background: linear-gradient(180deg, rgba(15, 23, 42, 0.7) 0%, rgba(15, 23, 42, 0.9) 30%, #0F172A 100%);
        }
        @media (min-width: 1024px) { /* lg breakpoint */
            .transcript-panel {
                background: #1e293b; /* Slate 800 */
            }
        }

        .transcript-area::-webkit-scrollbar { display: none; }
        .transcript-area { -ms-overflow-style: none; scrollbar-width: none; }


        .transcript-actor-ai { color: #818CF8; font-weight: 600; }
        .transcript-actor-candidate { color: #34D399; font-weight: 600; }

        .hidden-section { display: none !important; }

        .mic-listening i {
            color: #F87171; /* Red 400 */
            animation: pulse 1.5s infinite;
        }

        .primary-btn {
            background: linear-gradient(to right, #4F46E5, #7C3AED);
            transition: all 0.3s ease;
        }
        .primary-btn:hover {
            box-shadow: 0 0 20px rgba(99, 102, 241, 0.5);
            transform: translateY(-2px);
        }
        .primary-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .timer-progress-circle {
            transition: stroke-dashoffset 1s linear;
        }

        @keyframes pulse {
            0% { transform: scale(1); box-shadow: 0 0 0 0 rgba(248, 113, 113, 0.7); }
            70% { transform: scale(1.1); box-shadow: 0 0 10px 10px rgba(248, 113, 113, 0); }
            100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(248, 113, 113, 0); }
        }

        .loader {
            width: 48px; height: 48px; border: 3px solid #4F46E5;
            border-bottom-color: transparent; border-radius: 50%;
            display: inline-block; box-sizing: border-box;
            animation: rotation 1s linear infinite;
        }
        .loader-small {
            width: 24px; height: 24px; border: 2px solid #6366F1;
            border-bottom-color: transparent; border-radius: 50%;
            display: inline-block; animation: rotation 1s linear infinite;
        }

        @keyframes rotation {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="text-slate-200">

    <!-- Main Interview Layout -->
    <div id="interview-layout" class="hidden-section h-screen w-screen flex flex-col lg:flex-row">
        <!-- Video Panel -->
        <div class="video-wrapper lg:w-1/2 lg:h-full w-full h-full lg:relative absolute">
            <video id="camera-feed" class="w-full h-full object-cover" autoplay muted playsinline></video>
            <div class="absolute top-4 left-4">
                <div class="relative">
                    <img id="ai-avatar-img" src="https://placehold.co/80x80/1E293B/94A3B8?text=AI" alt="AI Interviewer" class="ai-avatar mx-auto rounded-full">
                    <div id="ai-status-icon" class="absolute -bottom-1 -right-1 bg-slate-700 rounded-full p-2 border-2 border-slate-900">
                        <i class="fas fa-brain text-indigo-400 text-xs"></i>
                    </div>
                </div>
                 <p id="ai-status" class="text-center font-semibold text-slate-300 text-xs mt-1">Initializing...</p>
            </div>
            <div class="absolute top-4 right-4 z-20">
                <button id="end-interview-button" class="bg-red-600/80 text-white py-2 px-4 rounded-lg hover:bg-red-600/100 transition-colors font-semibold flex items-center text-sm">
                    <i class="fas fa-phone-slash mr-2"></i> End
                </button>
            </div>
            <div id="timer-container" class="absolute bottom-4 right-4 text-center hidden-section lg:static lg:top-4 lg:left-auto">
                <div class="relative w-16 h-16">
                     <svg class="w-full h-full transform -rotate-90">
                        <circle class="text-slate-700" stroke-width="5" stroke="currentColor" fill="transparent" r="28" cx="32" cy="32"/>
                        <circle id="timer-progress" class="text-indigo-400 timer-progress-circle"
                            stroke-width="5" stroke-linecap="round" stroke="currentColor" fill="transparent" r="28" cx="32" cy="32"/>
                    </svg>
                    <span id="timer-text" class="absolute top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 font-bold text-lg">90</span>
                </div>
            </div>
        </div>

        <!-- Transcript Panel -->
        <div class="lg:w-1/2 w-full h-1/2 lg:h-full p-4 sm:p-6 flex flex-col transcript-panel absolute bottom-0 lg:static z-20">
            <div class="mb-4 flex-shrink-0">
                 <h1 id="info-job-title" class="text-xl sm:text-2xl font-bold text-white"></h1>
                 <p id="info-company-name" class="text-slate-400 text-sm"></p>
            </div>
            <div id="transcript-area" class="transcript-area rounded-lg flex-grow p-4 mb-4 min-h-0 overflow-y-auto"></div>
            <div id="interaction-status-area" class="p-4 bg-slate-900/50 rounded-lg min-h-[70px] flex items-center justify-center text-center flex-shrink-0">
                <div id="interaction-status-text" class="text-slate-300 font-medium text-lg"></div>
                <!-- Desktop speak button -->
                <button id="speak-button" class="hidden-section primary-btn w-full text-white font-bold py-3 px-4 rounded-lg flex items-center justify-center text-lg">
                    <i class="fas fa-microphone-alt mr-3"></i> Tap to Speak
                </button>
            </div>
        </div>
    </div>

    <!-- Overlays -->
    <div id="loading-overlay" class="modal-overlay fixed inset-0 z-50 flex items-center justify-center">
        <div class="text-center"><div class="loader mx-auto"></div><p class="text-lg text-slate-400 mt-6">Loading...</p></div>
    </div>
    <div id="error-overlay" class="hidden-section modal-overlay fixed inset-0 z-50 flex items-center justify-center p-4">
        <div class="glass-card rounded-2xl p-8 text-center max-w-md"><i class="fas fa-exclamation-triangle text-red-400 fa-3x mb-4"></i><h2 class="text-2xl font-semibold text-red-400 mb-2">Error</h2><p id="error-message" class="text-slate-300"></p><button onclick="window.location.reload()" class="primary-btn mt-8 text-white font-bold py-2 px-6 rounded-lg">Try Again</button></div>
    </div>
    <div id="permissions-overlay" class="hidden-section modal-overlay fixed inset-0 z-50 flex items-center justify-center p-4">
         <div class="glass-card rounded-2xl p-8 text-center max-w-lg">
             <i class="fas fa-shield-halved text-indigo-400 text-5xl mb-6"></i>
             <h2 class="text-3xl font-bold text-white mb-2">Permissions Required</h2>
             <p id="permissions-text" class="text-slate-400 mb-8">This interview requires microphone access to proceed.</p>
             <button id="grant-permissions-btn" class="primary-btn w-full text-white font-bold py-3 px-4 rounded-lg flex items-center justify-center text-lg">Grant Access</button>
         </div>
    </div>
    <div id="instructions-overlay" class="hidden-section modal-overlay fixed inset-0 z-50 flex items-center justify-center p-4">
         <div class="glass-card rounded-2xl p-8 text-center max-w-lg">
             <i class="fas fa-info-circle text-indigo-400 text-5xl mb-6"></i>
             <h2 class="text-3xl font-bold text-white mb-4">Interview Instructions</h2>
             <div class="text-left space-y-4 text-slate-300">
                 <p>You will be speaking with <strong>Alex</strong>, our AI interviewer. Please review the following guidelines:</p>
                 <ul class="space-y-3">
                     <li class="flex items-start">
                         <i class="fas fa-headset w-6 text-indigo-400 mt-1"></i>
                         <span>Find a quiet, well-lit place.</span>
                     </li>
                     <li class="flex items-start">
                         <i class="fas fa-microphone w-6 text-indigo-400 mt-1"></i>
                         <span>On mobile, tap the microphone button to speak. On desktop, the mic is automatic.</span>
                     </li>
                     <li class="flex items-start">
                         <i class="fas fa-eye w-6 text-indigo-400 mt-1"></i>
                         <span>Keep your camera on and stay visible.</span>
                     </li>
                     <li class="flex items-start">
                         <i class="fas fa-window-maximize w-6 text-indigo-400 mt-1"></i>
                         <span>Stay in this browser tab to avoid ending the interview.</span>
                     </li>
                 </ul>
             </div>
             <button id="start-details-btn" class="primary-btn mt-8 w-full text-white font-bold py-3 px-4 rounded-lg text-lg">I Understand, Continue</button>
         </div>
    </div>
    <div id="welcome-overlay" class="hidden-section modal-overlay fixed inset-0 z-50 flex items-center justify-center p-4">
        <div class="glass-card rounded-2xl p-8 text-center max-w-lg"><h1 class="text-3xl font-bold text-white mb-4">Final Step</h1><p class="text-slate-400 mb-8">Your setup is complete. Please confirm your details to begin.</p><form id="candidate-details-form" class="space-y-6 text-left"><style>.form-input { background: #1e293b; border: 1px solid #334155; color: #cbd5e1; } .form-input:focus { border-color: #4f46e5; box-shadow: 0 0 0 1px #4f46e5; }</style><div><label for="candidateName" class="block text-sm font-medium text-slate-300 mb-1">Full Name</label><input type="text" id="candidateName" name="candidateName" required class="form-input w-full px-4 py-2 rounded-lg focus:outline-none"></div><div><label for="candidateEmail" class="block text-sm font-medium text-slate-300 mb-1">Email Address</label><input type="email" id="candidateEmail" name="candidateEmail" required class="form-input w-full px-4 py-2 rounded-lg focus:outline-none"></div><div><label for="resumeFile" class="block text-sm font-medium text-slate-300 mb-1">Upload Resume</label><input type="file" id="resumeFile" name="resumeFile" required accept=".pdf,.doc,.docx,.txt" class="w-full text-sm text-slate-400 file:mr-4 file:py-2 file:px-4 file:rounded-full file:border-0 file:text-sm file:font-semibold file:bg-indigo-500/10 file:text-indigo-300 hover:file:bg-indigo-500/20"></div><div id="submit-details-error" class="hidden-section text-red-400 text-sm"></div><button type="submit" id="submit-details-button" class="primary-btn w-full text-white font-bold py-3 px-4 rounded-lg flex items-center justify-center text-lg">Start Interview <i class="fas fa-arrow-right ml-3"></i></button></form></div>
    </div>
    <div id="ended-overlay" class="hidden-section modal-overlay fixed inset-0 z-50 flex items-center justify-center p-4">
        <div class="glass-card rounded-2xl p-12 text-center max-w-md"><i class="fas fa-check-circle text-emerald-400 text-5xl mb-6"></i><h2 class="text-3xl font-bold text-white mb-2">Interview Concluded</h2><p class="text-slate-400">Thank you for your time. The hiring team will be in touch regarding the next steps.</p></div>
    </div>

<script>
    document.addEventListener('DOMContentLoaded', () => {
        // --- CONFIGURATION ---
        const API_BASE_URL = window.location.origin;
        const ANSWER_TIME_LIMIT = 90;
        const SCREENSHOT_INTERVAL_MS = 5000;

        // --- STATE VARIABLES ---
        let recognition = null;
        let isListening = false;
        let currentInterviewId = null;
        let cameraStream = null;
        let audioContext = null;
        let currentStreamingTextElement = null;
        let screenshotInterval = null;
        let masterTimer;
        let visualTimerInterval;
        let speechTimeout;
        let isMobile = false;

        // --- DOM ELEMENT CACHE ---
        const domElements = {
            interviewLayout: document.getElementById('interview-layout'),
            cameraFeed: document.getElementById('camera-feed'),
            aiAvatarImg: document.getElementById('ai-avatar-img'),
            aiStatus: document.getElementById('ai-status'),
            aiStatusIcon: document.getElementById('ai-status-icon'),
            infoJobTitleEl: document.getElementById('info-job-title'),
            infoCompanyNameEl: document.getElementById('info-company-name'),
            transcriptArea: document.getElementById('transcript-area'),
            interactionStatusArea: document.getElementById('interaction-status-area'),
            interactionStatusText: document.getElementById('interaction-status-text'),
            speakButton: document.getElementById('speak-button'),
            endInterviewButton: document.getElementById('end-interview-button'),
            grantPermissionsBtn: document.getElementById('grant-permissions-btn'),
            startDetailsBtn: document.getElementById('start-details-btn'),
            candidateDetailsForm: document.getElementById('candidate-details-form'),
            submitDetailsErrorEl: document.getElementById('submit-details-error'),
            overlays: {
                loading: document.getElementById('loading-overlay'),
                error: document.getElementById('error-overlay'),
                permissions: document.getElementById('permissions-overlay'),
                instructions: document.getElementById('instructions-overlay'),
                welcome: document.getElementById('welcome-overlay'),
                ended: document.getElementById('ended-overlay')
            },
            timerContainer: document.getElementById('timer-container'),
            timerText: document.getElementById('timer-text'),
            timerProgress: document.getElementById('timer-progress')
        };

        // --- HELPER FUNCTIONS ---
        const isMobileDevice = () => {
            // More comprehensive mobile detection
            const userAgent = navigator.userAgent.toLowerCase();
            const mobileKeywords = [
                'android', 'webos', 'iphone', 'ipad', 'ipod', 'blackberry', 
                'iemobile', 'opera mini', 'mobile', 'tablet'
            ];
            
            const isMobileDetected = mobileKeywords.some(keyword => userAgent.includes(keyword));
            
            // Additional check for touch capability
            const hasTouch = 'ontouchstart' in window || navigator.maxTouchPoints > 0;
            
            const mobile = isMobileDetected || hasTouch;
            isMobile = mobile;
            
            console.log('Mobile detection:', {
                userAgent: userAgent,
                isMobile: mobile,
                hasTouch: hasTouch,
                maxTouchPoints: navigator.maxTouchPoints
            });
            
            return mobile;
        };

        const showOverlay = (overlayId) => {
            Object.values(domElements.overlays).forEach(o => o.classList.add('hidden-section'));
            if (overlayId) domElements.overlays[overlayId].classList.remove('hidden-section');
        };

        const displayError = (message = "An unknown error occurred.") => {
            domElements.errorMessageEl.textContent = message;
            domElements.interviewLayout.classList.add('hidden-section');
            showOverlay('error');
            if (cameraStream) cameraStream.getTracks().forEach(track => track.stop());
            clearTimers();
        };

        function showTextInputFallback() {
            console.log('Text input fallback not available on desktop');
        }

        const appendToTranscript = (actor, text, isStreaming = false) => {
            const entry = document.createElement('div');
            entry.className = 'transcript-entry p-3 rounded-lg mb-2';

            const actorSpan = document.createElement('span');
            actorSpan.className = actor === 'ai' ? 'transcript-actor-ai' : 'transcript-actor-candidate';
            actorSpan.textContent = actor === 'ai' ? 'Alex (AI): ' : 'You: ';

            const textEl = document.createElement('p');
            textEl.className = 'text-slate-300 mt-1 inline';

            entry.appendChild(actorSpan);
            entry.appendChild(textEl);

            if (isStreaming) {
                currentStreamingTextElement = textEl;
            } else {
                textEl.textContent = text;
                currentStreamingTextElement = null;
            }

            domElements.transcriptArea.appendChild(entry);
            domElements.transcriptArea.scrollTop = domElements.transcriptArea.scrollHeight;
        };

        const streamText = (text, audioDuration) => {
            if (!currentStreamingTextElement) return;
            const words = text.split(/\s+/);
            const timePerWord = (audioDuration * 1000) / words.length;
            let i = 0;
            const intervalId = setInterval(() => {
                if (i < words.length) {
                    currentStreamingTextElement.textContent += (i > 0 ? ' ' : '') + words[i];
                    domElements.transcriptArea.scrollTop = domElements.transcriptArea.scrollHeight;
                    i++;
                } else {
                    clearInterval(intervalId);
                }
            }, timePerWord);
        };

        const setInteractionStatus = (status, text) => {
            const { aiStatusIcon, aiStatus, aiAvatarImg, interactionStatusText, speakButton, timerContainer } = domElements;

            clearTimers();
            aiAvatarImg.classList.remove('speaking');
            speakButton.classList.add('hidden-section');
            interactionStatusText.classList.remove('hidden-section');

            switch (status) {
                case 'speaking':
                    aiStatus.textContent = 'Speaking';
                    interactionStatusText.textContent = text || 'AI is speaking...';
                    aiStatusIcon.innerHTML = `<i class="fas fa-volume-up text-indigo-400 text-xs"></i>`;
                    aiAvatarImg.classList.add('speaking');
                    break;
                case 'listening':
                    aiStatus.textContent = 'Listening';
                    interactionStatusText.textContent = text || 'Listening for your response...';
                    aiStatusIcon.innerHTML = `<i class="fas fa-microphone-alt text-indigo-400 text-xs"></i>`;
                    speakButton.classList.remove('hidden-section');
                    speakButton.innerHTML = '<i class="fas fa-stop-circle mr-3"></i> Stop Speaking';
                    // Auto-start speech recognition on desktop
                    if (recognition && !isListening) {
                        setTimeout(() => {
                            startSpeechRecognition();
                        }, 1000);
                    }
                    break;
                case 'processing':
                    aiStatus.textContent = 'Processing';
                    interactionStatusText.textContent = text || 'Processing your response...';
                    aiStatusIcon.innerHTML = `<i class="fas fa-brain text-indigo-400 text-xs animate-pulse"></i>`;
                    break;
                case 'waiting':
                    aiStatus.textContent = 'Waiting';
                    interactionStatusText.textContent = text || 'Please wait.';
                    aiStatusIcon.innerHTML = `<i class="fas fa-pause text-slate-400 text-xs"></i>`;
                    speakButton.classList.remove('hidden-section');
                    speakButton.innerHTML = '<i class="fas fa-microphone-alt mr-3"></i> Tap to Speak';
                    break;
                case 'error':
                    aiStatus.textContent = 'Error';
                    interactionStatusText.textContent = text || 'An error occurred.';
                    aiStatusIcon.innerHTML = `<i class="fas fa-exclamation-triangle text-red-400 text-xs"></i>`;
                    speakButton.classList.remove('hidden-section');
                    speakButton.innerHTML = '<i class="fas fa-microphone-alt mr-3"></i> Try Again';
                    break;
                default:
                    aiStatus.textContent = 'Ready';
                    interactionStatusText.textContent = text || 'Ready to start.';
                    aiStatusIcon.innerHTML = `<i class="fas fa-check text-green-400 text-xs"></i>`;
            }
        };

        // --- TIMER FUNCTIONS ---
        const startTimers = () => {
            const { timerContainer, timerText, timerProgress } = domElements;
            const radius = parseFloat(timerProgress.getAttribute('r'));
            const circumference = 2 * Math.PI * radius;
            let timeLeft = ANSWER_TIME_LIMIT;

            timerContainer.classList.remove('hidden-section');
            timerProgress.style.strokeDasharray = circumference;

            const updateTimerDisplay = () => {
                timerText.textContent = timeLeft;
                const offset = circumference - (timeLeft / ANSWER_TIME_LIMIT) * circumference;
                timerProgress.style.strokeDashoffset = offset;
            };

            updateTimerDisplay();

            masterTimer = setTimeout(() => {
                if (isListening && recognition) {
                    recognition.stop();
                }
            }, ANSWER_TIME_LIMIT * 1000);

            visualTimerInterval = setInterval(() => {
                timeLeft--;
                updateTimerDisplay();
                if (timeLeft <= 0) {
                    clearInterval(visualTimerInterval);
                }
            }, 1000);
        };

        const clearTimers = () => {
            clearTimeout(masterTimer);
            clearInterval(visualTimerInterval);
            if(domElements.timerContainer) domElements.timerContainer.classList.add('hidden-section');
        };

        // --- CORE LOGIC FUNCTIONS ---
        const setupCamera = async () => {
            try {
                console.log('Setting up camera and microphone...');
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    video: true, 
                    audio: true 
                });
                
                cameraStream = stream;
                domElements.cameraFeed.srcObject = stream;
                domElements.cameraFeed.style.display = 'block';
                
                console.log('Camera and microphone setup completed');
                return true;
            } catch (err) {
                console.error("Permission error:", err);
                displayError("Camera and microphone permissions are required. Please grant access and refresh the page.");
                return false;
            }
        };

        const captureAndSendScreenshot = async () => {
            if (!cameraStream || !currentInterviewId) return;
            
            // Check if we have video tracks
            const videoTracks = cameraStream.getVideoTracks();
            if (videoTracks.length === 0) {
                console.log('No video tracks available, skipping screenshot');
                return;
            }
            
            const canvas = document.createElement('canvas');
            const video = domElements.cameraFeed;
            
            // Check if video is actually playing
            if (video.readyState < 2) {
                console.log('Video not ready, skipping screenshot');
                return;
            }
            
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageDataUrl = canvas.toDataURL('image/jpeg', 0.8);
            try {
                fetch(`${API_BASE_URL}/${currentInterviewId}/screenshot`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ image: imageDataUrl })
                });
            } catch (error) {
                console.error("Failed to send screenshot:", error);
            }
        };

        const playQuestionAudio = (text) => {
            return new Promise(async (resolve) => {
                setInteractionStatus('speaking');
                try {
                    const response = await fetch(`${API_BASE_URL}/text-to-speech`, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ text })
                    });
                    if (!response.ok) throw new Error('Failed to fetch audio.');

                    const audioData = await response.arrayBuffer();
                    if (!audioContext) {
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    }
                    
                    // Resume audio context if suspended (required for mobile)
                    if (audioContext.state === 'suspended') {
                        await audioContext.resume();
                    }

                    const audioBuffer = await audioContext.decodeAudioData(audioData);
                    streamText(text, audioBuffer.duration);

                    const source = audioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(audioContext.destination);
                    source.start(0);
                    source.onended = resolve;
                } catch (error) {
                    console.error("Text-to-speech error:", error);
                    if(currentStreamingTextElement) currentStreamingTextElement.textContent = text;
                    setTimeout(resolve, 3000);
                }
            });
        };

        const submitResponse = async (responseText) => {
            clearTimers();
            setInteractionStatus('processing');
            appendToTranscript('candidate', responseText);

            try {
                const response = await fetch(`${API_BASE_URL}/${currentInterviewId}/next-question`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ response_text: responseText })
                });
                if (!response.ok) {
                   const errorData = await response.json().catch(()=> ({message: "Server returned an error."}));
                   throw new Error(errorData.message);
                }
                const data = await response.json();

                if (data.interview_status === 'Completed') {
                    await playQuestionAudio(data.question.text);
                    setInteractionStatus('waiting', 'Interview complete. Thank you.');
                    showOverlay('ended');
                    if (cameraStream) cameraStream.getTracks().forEach(track => track.stop());
                } else {
                    await askQuestion(data.question.text);
                }
            } catch (error) {
                console.error("Error submitting response:", error);
                setInteractionStatus('error', "Error processing response. Recovering...");
                setTimeout(() => startInterviewFlow(), 5000);
            }
        };

        const askQuestion = async (questionText) => {
            appendToTranscript('ai', '', true);
            await playQuestionAudio(questionText);
            setInteractionStatus('listening');
        };

        const startInterviewFlow = async () => {
            showOverlay(null);
            domElements.interviewLayout.classList.remove('hidden-section');
            setInteractionStatus('processing', 'Preparing first question...');
            if (screenshotInterval) clearInterval(screenshotInterval);
            screenshotInterval = setInterval(captureAndSendScreenshot, SCREENSHOT_INTERVAL_MS);

            try {
                const response = await fetch(`${API_BASE_URL}/${currentInterviewId}/start`, { method: 'POST' });
                if (!response.ok) {
                    const err = await response.json().catch(() => ({ message: `HTTP error ${response.status}` }));
                    throw new Error(err.message || 'Failed to start interview.');
                }
                const data = await response.json();
                await askQuestion(data.question.text);
            } catch (error) {
                displayError(`Could not start interview: ${error.message}`);
            }
        };

        // --- SPEECH RECOGNITION FUNCTIONS ---
        const startSpeechRecognition = () => {
            if (!recognition) {
                console.error('Speech recognition not initialized');
                return;
            }
            
            try {
                // Resume audio context if needed
                if (audioContext && audioContext.state === 'suspended') {
                    audioContext.resume();
                }
                
                // Clear any existing state
                isListening = false;
                
                // Reset recognition if it's already running
                try {
                    recognition.stop();
                } catch (e) {
                    // Ignore errors if not running
                }
                
                // Start fresh after a short delay
                setTimeout(() => {
                    try {
                        recognition.start();
                        isListening = true;
                        console.log('Speech recognition started successfully');
                    } catch (error) {
                        console.error('Error starting speech recognition:', error);
                        setInteractionStatus('error', 'Failed to start microphone. Please try again.');
                    }
                }, 200);
                
            } catch (error) {
                console.error('Error in startSpeechRecognition:', error);
                setInteractionStatus('error', 'Failed to start microphone. Please try again.');
            }
        };

        const stopSpeechRecognition = () => {
            if (!recognition) return;
            
            try {
                recognition.stop();
                isListening = false;
                console.log('Speech recognition stopped');
            } catch (error) {
                console.error('Error stopping speech recognition:', error);
            }
        };

        // --- INITIALIZATION ---
        function setupEventListeners() {
            domElements.grantPermissionsBtn.addEventListener('click', async () => {
                domElements.grantPermissionsBtn.disabled = true;
                if (await setupCamera()) {
                    showOverlay('instructions');
                }
                domElements.grantPermissionsBtn.disabled = false;
            });

            domElements.startDetailsBtn.addEventListener('click', () => {
                showOverlay('welcome');
            });

            domElements.endInterviewButton.addEventListener('click', async () => {
                if (confirm("Are you sure you want to end the interview?")) {
                    if (cameraStream) cameraStream.getTracks().forEach(track => track.stop());
                    clearTimers();
                    showOverlay('ended');
                    try {
                        fetch(`${API_BASE_URL}/${currentInterviewId}/end`, { method: 'POST' });
                    } catch (error) { console.error("Failed to notify backend of manual end:", error); }
                }
            });

            domElements.candidateDetailsForm.addEventListener('submit', async (e) => {
                e.preventDefault();
                if (!currentInterviewId) return displayError("Interview ID is missing.");
                const submitButton = e.target.querySelector('button[type="submit"]');
                submitButton.disabled = true;

                const formData = new FormData(domElements.candidateDetailsForm);

                try {
                    const response = await fetch(`${API_BASE_URL}/${currentInterviewId}/submit-details`, {
                        method: 'POST',
                        body: formData
                    });
                    if (!response.ok) {
                        const err = await response.json().catch(() => ({ message: `HTTP error ${response.status}` }));
                        throw new Error(err.message || 'Failed to submit details.');
                    }
                    await startInterviewFlow();
                } catch (error) {
                    domElements.submitDetailsErrorEl.textContent = error.message;
                    domElements.submitDetailsErrorEl.classList.remove('hidden-section');
                } finally {
                    submitButton.disabled = false;
                }
            });

            // Desktop speak button
            domElements.speakButton.addEventListener('click', () => {
                console.log('Desktop speak button clicked, isListening:', isListening);
                if (!isListening && recognition) {
                    startSpeechRecognition();
                    domElements.speakButton.innerHTML = '<i class="fas fa-stop-circle mr-3"></i> Stop Speaking';
                } else if(isListening && recognition) {
                    stopSpeechRecognition();
                    domElements.speakButton.innerHTML = '<i class="fas fa-microphone-alt mr-3"></i> Tap to Speak';
                }
            });
        }

        function initializeRecognition() {
            console.log('=== INITIALIZING SPEECH RECOGNITION ===');
            
            // Check for HTTPS first
            if (!checkHTTPS()) {
                console.error('HTTPS check failed during initialization');
                return;
            }
            
            // Check for speech recognition support
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!SpeechRecognition) {
                console.error('❌ Speech Recognition not supported');
                console.log('Available APIs:', {
                    SpeechRecognition: !!window.SpeechRecognition,
                    webkitSpeechRecognition: !!window.webkitSpeechRecognition,
                    userAgent: navigator.userAgent
                });
                
                displayError("Speech Recognition is not supported by this browser.");
                return;
            }

            console.log('✅ Speech Recognition API found');
            
            try {
                recognition = new SpeechRecognition();
                
                // Desktop configuration
                recognition.continuous = false;
                recognition.interimResults = true;
                recognition.lang = 'en-US';
                recognition.maxAlternatives = 1;

                console.log('Speech recognition initialized with settings:', {
                    continuous: recognition.continuous,
                    interimResults: recognition.interimResults,
                    lang: recognition.lang,
                    userAgent: navigator.userAgent
                });

                // Set up event handlers
                setupRecognitionEventHandlers();
                
                console.log('✅ Speech recognition initialization completed successfully');
            } catch (error) {
                console.error('❌ Error creating SpeechRecognition instance:', error);
                setInteractionStatus('error', 'Failed to initialize speech recognition.');
            }
        }

        function setupRecognitionEventHandlers() {
            recognition.onstart = () => {
                isListening = true;
                console.log('✅ Speech recognition started - listening for speech');
                setInteractionStatus('listening', 'Listening... Speak now!');
            };

            recognition.onend = () => {
                isListening = false;
                console.log('Speech recognition ended');
                setInteractionStatus('waiting', 'Please speak again');
            };

            recognition.onresult = (event) => {
                console.log('Speech recognition result received:', event);
                console.log('Number of results:', event.results.length);
                
                let finalTranscript = '';
                let interimTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    const transcript = event.results[i][0].transcript;
                    console.log(`Result ${i}:`, transcript, 'isFinal:', event.results[i].isFinal);
                    
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript;
                    } else {
                        interimTranscript += transcript;
                    }
                }

                console.log('Final transcript:', finalTranscript);
                console.log('Interim transcript:', interimTranscript);

                // Show interim results
                if (interimTranscript || finalTranscript) {
                    domElements.interactionStatusText.innerHTML = 
                        `<span class="mic-listening"><i class="fas fa-microphone-alt fa-lg mr-3"></i></span> ${finalTranscript}${interimTranscript}`;
                }

                // Handle final results
                if (finalTranscript.trim().length > 3) {
                    console.log('Submitting response:', finalTranscript.trim());
                    submitResponse(finalTranscript.trim());
                } else if (finalTranscript.trim().length > 0) {
                    console.log('Transcript too short, not submitting:', finalTranscript.trim());
                }
            };

            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error, event);
                isListening = false;
                
                // Handle specific errors
                switch (event.error) {
                    case 'no-speech':
                        console.log('No speech detected');
                        setInteractionStatus('waiting', 'No speech detected. Please try again.');
                        break;
                    case 'audio-capture':
                        console.error('Audio capture failed');
                        setInteractionStatus('error', 'Microphone access failed. Please check permissions.');
                        break;
                    case 'not-allowed':
                        console.error('Permission denied');
                        setInteractionStatus('error', 'Microphone permission denied. Please grant access.');
                        break;
                    case 'network':
                        console.error('Network error');
                        setInteractionStatus('error', 'Network error. Please check your connection.');
                        break;
                    case 'aborted':
                        console.log('Recognition aborted');
                        break;
                    default:
                        console.error('Unknown error:', event.error);
                        setInteractionStatus('error', 'Speech recognition error. Please try again.');
                }
            };

            recognition.onaudiostart = () => {
                console.log('Audio capturing started');
            };

            recognition.onaudioend = () => {
                console.log('Audio capturing ended');
            };

            recognition.onsoundstart = () => {
                console.log('Sound detected');
            };

            recognition.onsoundend = () => {
                console.log('Sound ended');
            };

            recognition.onspeechstart = () => {
                console.log('Speech started');
            };

            recognition.onspeechend = () => {
                console.log('Speech ended');
            };
        }

        // Check if running on HTTPS (required for microphone access on mobile)
        function checkHTTPS() {
            const isHTTPS = window.location.protocol === 'https:' || window.location.hostname === 'localhost';
            console.log('HTTPS check:', {
                protocol: window.location.protocol,
                hostname: window.location.hostname,
                isHTTPS: isHTTPS
            });
            
            if (!isHTTPS) {
                console.error('❌ HTTPS required for microphone access');
                setInteractionStatus('error', 'HTTPS is required for microphone access. Please use a secure connection.');
                return false;
            }
            return true;
        }

        // Show mobile warning and hide all content
        function showMobileWarning() {
            const mobileWarning = `
                <div class="fixed inset-0 bg-slate-900 flex items-center justify-center z-50">
                    <div class="bg-slate-800 p-8 rounded-lg max-w-md mx-4 text-center">
                        <div class="text-red-400 text-6xl mb-4">
                            <i class="fas fa-mobile-alt"></i>
                        </div>
                        <h2 class="text-2xl font-bold text-white mb-4">Mobile Not Supported</h2>
                        <p class="text-slate-300 mb-6">
                            This AI interview application is currently optimized for desktop computers only. 
                            Please switch to a desktop or laptop computer for the best experience.
                        </p>
                        <div class="text-slate-400 text-sm">
                            <i class="fas fa-info-circle mr-2"></i>
                            Mobile support will be available in a future update.
                        </div>
                    </div>
                </div>
            `;
            document.body.innerHTML = mobileWarning;
        }

        async function main() {
            try {
                showOverlay('loading');
                console.log('Starting application initialization...');
                console.log('User Agent:', navigator.userAgent);
                console.log('Platform:', navigator.platform);
                console.log('Protocol:', window.location.protocol);
                console.log('Hostname:', window.location.hostname);
                
                // Check if mobile device and show warning
                if (isMobileDevice()) {
                    console.log('Mobile device detected - showing warning');
                    showMobileWarning();
                    return;
                }
                
                // Add timeout to prevent stuck loading
                const timeoutId = setTimeout(() => {
                    console.error('Initialization timeout - taking too long');
                    displayError("Initialization is taking too long. Please refresh the page and try again.");
                }, 30000); // 30 second timeout
                
                // Check for required APIs
                console.log('Speech Recognition available:', !!(window.SpeechRecognition || window.webkitSpeechRecognition));
                console.log('MediaDevices available:', !!navigator.mediaDevices);
                console.log('getUserMedia available:', !!navigator.mediaDevices?.getUserMedia);
                
                setupEventListeners();
                console.log('Event listeners setup completed');
                
                initializeRecognition();
                console.log('Speech recognition initialization completed');

                const urlParams = new URLSearchParams(window.location.search);
                const invitationLink = urlParams.get('invite');
                console.log('Invitation link from URL:', invitationLink);

                if (!invitationLink) {
                    clearTimeout(timeoutId);
                    displayError("No invitation link found. Please use the link provided in your invitation.");
                    return;
                }

                console.log('Fetching interview data for invitation:', invitationLink);
                console.log('API URL:', `${API_BASE_URL}/initiate/${invitationLink}`);
                
                const response = await fetch(`${API_BASE_URL}/initiate/${invitationLink}`);
                console.log('Response status:', response.status);
                console.log('Response ok:', response.ok);
                
                if (!response.ok) {
                    const err = await response.json().catch(() => ({ message: `HTTP error ${response.status}` }));
                    console.error('API Error:', err);
                    clearTimeout(timeoutId);
                    throw new Error(err.message || 'Failed to load interview.');
                }
                
                const data = await response.json();
                console.log('Interview data received:', data);
                
                currentInterviewId = data.interview_id;
                domElements.infoJobTitleEl.textContent = data.job_title || 'N/A';
                domElements.infoCompanyNameEl.textContent = `with ${data.company_name || 'N/A'}`;
                
                // Set permissions text
                const permissionsText = document.getElementById('permissions-text');
                permissionsText.textContent = 'This interview requires camera and microphone access to proceed.';
                
                clearTimeout(timeoutId);
                console.log('Showing permissions overlay');
                showOverlay('permissions');
                
            } catch (error) {
                console.error('Error during initialization:', error);
                console.error('Error stack:', error.stack);
                displayError(error.message);
            }
        }

        main();
    });
</script>
</body>
</html>

